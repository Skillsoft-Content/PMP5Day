WEBVTT

1
00:00:00.149 --> 00:00:00.870
together.

2
00:00:03.600 --> 00:00:11.340
ground rules, as defined in the team charter clear expectations set regarding the code of conduct for TEAM members.

3
00:00:14.790 --> 00:00:37.380
Expert judgment judgment based on expertise in an application area knowledge area discipline industry, etc, as appropriate for the activity being performed such expertise may be provided by any group or person with specialized education knowledge skill experience or training.

4
00:00:40.470 --> 00:00:48.390
resource calendars identify working days shifts and when specific resources are made available to the project.

5
00:00:51.480 --> 00:01:01.020
Lessons learned register a project document use to record knowledge gained during a project so that it can be used in the current project and.

6
00:01:04.890 --> 00:01:13.500
**Susan Daniels**: hey you We are halfway through this five day boot camp let's do a little happy dance right we've made it halfway.

7
00:01:15.210 --> 00:01:15.990
**Susan Daniels**: i'm excited.

8
00:01:17.220 --> 00:01:37.260
**Susan Daniels**: So let's talk about what's going to be on on the agenda for this afternoon, so this first segment that we're recording now is really going to focus on risk management and then we'll take a break, and then the afternoon is gonna be kind of a list of multiple things that we will be covering.

9
00:01:38.310 --> 00:01:40.650
**Susan Daniels**: let's go ahead and take a look at our agenda.

10
00:01:41.760 --> 00:01:49.410
**Susan Daniels**: So assassin manage risk is the first thing we're going to focus on and we're going to spend pretty much an hour focused on risk management.

11
00:01:50.010 --> 00:01:58.830
**Susan Daniels**: more about that later, but, as you can see, there are other aspects and we're going to go back over things we were planning when we're trying to start the project.

12
00:01:59.100 --> 00:02:18.030
**Susan Daniels**: Now we're in doing the work so it's the executing processes and tools and techniques that will use in these spaces, so we have lots of topics will cover this afternoon, so that's sort of an overview of where we are on day three afternoons on sessions first segment.

13
00:02:20.880 --> 00:02:30.030
**Susan Daniels**: This is the same thing I just showed you is just introductory slide from pmi and let's get started in assessing manage risk so we're going to spend.

14
00:02:30.510 --> 00:02:56.370
**Susan Daniels**: almost an hour talking about risk management, because honestly as project managers um, that is what we've got to really constantly be aware of and we're trying to mitigate and minimize we're trying to transfer the risk we have various strategies that we use in our managing risk.

15
00:02:57.420 --> 00:03:16.050
**Susan Daniels**: And we're going to look at some tools and techniques that we we employ in this space, I think, after this hour you're going to walk away with that's cool Okay, there is a lot of things that we have in play that really helped us as project managers protect our project okay.

16
00:03:17.940 --> 00:03:21.480
**Susan Daniels**: So the first thing we need to start with is risk right.

17
00:03:22.860 --> 00:03:30.540
**Susan Daniels**: Well, in English, commonly risk always kind of refers to negative things things that.

18
00:03:31.050 --> 00:03:46.320
**Susan Daniels**: could happen to us in a bad way well in pmi in their lexicon risk can be anything that could impact your project either positively or negatively.

19
00:03:46.830 --> 00:04:00.090
**Susan Daniels**: So risk is any impact to your project Okay, anyone can impact it if it's positive we refer to it as an opportunity and we often want to exploit some of these are enhanced the probability okay.

20
00:04:03.600 --> 00:04:13.890
**Susan Daniels**: If it is a negative risk, it is a threat Okay, so we want to capture any threats and have strategies and plans in place for those.

21
00:04:18.270 --> 00:04:35.700
**Susan Daniels**: So what we're going to study here is how do we develop a risk management approach, and it really has to do with assessing the likelihood of something happening and the potential impact is going to be so key words associated with risk management.

22
00:04:36.840 --> 00:04:45.720
**Susan Daniels**: Much like I said archive is your key word for closing okay well one of your keywords for risk management, our risk are our.

23
00:04:46.620 --> 00:05:07.320
**Susan Daniels**: Is probability and impact okay probability and impact our keywords in the risk management domain, in fact, they have a whole word map just to emphasize and when asking project managers about risk, these are the two words that come to the to the four.

24
00:05:10.440 --> 00:05:22.440
**Susan Daniels**: Oh look we're going to do a risk management plan surprise right, so the thing becomes what goes in this risk management plan here's your list.

25
00:05:25.020 --> 00:05:33.360
**Susan Daniels**: But it's the same classic approach hey I need to identify all of my risks here's the people that i'm going to engage.

26
00:05:34.020 --> 00:05:41.700
**Susan Daniels**: they'll tell you whether you got the right people in the room right i'm going to assess each one of my stakeholders, for their risk appetite.

27
00:05:42.360 --> 00:05:58.860
**Susan Daniels**: Okay, by that is meant how much how well do they handle risk when give you an example on one end of the spectrum might be somebody that loves to play high stakes poker or loves to gamble Okay, they like risk.

28
00:05:59.820 --> 00:06:14.040
**Susan Daniels**: On the other end is somebody who is very risk averse Okay, and they don't they get very nervous when there is something that's ambiguous or could potentially go wrong.

29
00:06:14.790 --> 00:06:26.640
**Susan Daniels**: I put the other end would be you may be familiar with this expression on people who love to control hiring controlling people okay so.

30
00:06:27.210 --> 00:06:38.580
**Susan Daniels**: You need to understand where your stakeholders are for on on that continuum from controlling to really engage and prefer risk that's what is meant by risk appetite.

31
00:06:39.930 --> 00:06:44.550
**Susan Daniels**: we're going to look at what a probability and impact matrix looks like we're going to.

32
00:06:45.570 --> 00:06:56.340
**Susan Daniels**: learn how to use it in the next couple of days, but you're also going to show your your sponsors in this plan hey here's what my risk register looks like here's here's how i'm having.

33
00:06:56.760 --> 00:07:11.460
**Susan Daniels**: To help created this is where i'm going to store it, this is how often i'm going to update it right, which I would encourage to be no less than weekly, it should be an agenda item at least once a week okay.

34
00:07:11.820 --> 00:07:23.520
**Susan Daniels**: i'm probably more depending on your project Okay, and you tell them all this in your your risk management plan that you send up to senior leadership.

35
00:07:25.770 --> 00:07:33.630
**Susan Daniels**: So these are your elements, you can find more information about what goes into risk management plan explained more by.

36
00:07:34.140 --> 00:07:53.130
**Susan Daniels**: referencing that section in your pen box sixth or seventh edition, the seventh edition is going to take you to pick PM standards and it's going to give you articles and videos relative to risks Okay, and you can find a lot of narrative explanation in your sixth edition.

37
00:07:54.960 --> 00:08:01.350
**Susan Daniels**: So what the first thing we start out with is risk identification like, how do we go about.

38
00:08:02.280 --> 00:08:14.520
**Susan Daniels**: identifying what all could really get me out there right what could possibly causes project to go off track and are there really any opportunities out there.

39
00:08:15.030 --> 00:08:24.480
**Susan Daniels**: along this execution plan that are going to help me right could good things happen along the way right can good things happen, and what would those me okay.

40
00:08:29.190 --> 00:08:44.430
**Susan Daniels**: Okay, so let's go ahead and watch this video and i'm going to watch it watch start to 613 i'm not going to start at 613 i'm going to watch to 613 okay.

41
00:08:53.250 --> 00:08:58.470
**Susan Daniels**: wouldn't it be nice to know exactly what issues will face as we manage our projects.

42
00:08:58.920 --> 00:09:07.650
**Susan Daniels**: Unfortunately, we don't have a crystal ball that lets us see into the future of our projects, but we can't always wait to see what the future holds.

43
00:09:08.010 --> 00:09:13.410
**Susan Daniels**: In order to move forward on our projects will need to perform and assumptions analysis.

44
00:09:13.980 --> 00:09:23.160
**Susan Daniels**: And assumption is any factor in the planning process that's considered to be true real or certain but without proof, or demonstration.

45
00:09:23.700 --> 00:09:36.420
**Susan Daniels**: and assumptions analysis is a technique that explores the accuracy of assumptions and identifies risks to the project from being inaccurate inconsistent or incomplete with our assumptions.

46
00:09:36.930 --> 00:09:43.500
**Susan Daniels**: We have to make certain assumptions during planning, for example, the prices of raw materials will remain steady.

47
00:09:43.920 --> 00:09:57.510
**Susan Daniels**: vendors will follow through on their commitments and resources will remain available inaccurate assumptions contribute to risk, so we should regularly monitor our assumptions to determine if they prove to be correct or not.

48
00:09:58.500 --> 00:10:05.700
**Susan Daniels**: When working with a team to collect feedback or estimates sometimes the participants can influence one another.

49
00:10:06.180 --> 00:10:16.860
**Susan Daniels**: This is especially true if members of the management team are in attendance participants may hold back on their thoughts or opinions or they may be inclined to agree with the leader.

50
00:10:17.340 --> 00:10:27.840
**Susan Daniels**: Delphi technique can remove this bias with Delphi technique, the participation is anonymous and a facilitator should be present to assist with the activity.

51
00:10:28.290 --> 00:10:35.790
**Susan Daniels**: let's say there's a group of subject matter experts who are working together to assess the probability and impact of risk events.

52
00:10:36.330 --> 00:10:43.050
**Susan Daniels**: The experts will review the potential risks, then they'll provide their estimates or opinions anonymously.

53
00:10:43.620 --> 00:10:55.800
**Susan Daniels**: The facilitator will collect the responses and share them with the group, the Group will then discuss the results, often after the first round of voting, the estimates will be scattered across a wide range.

54
00:10:56.310 --> 00:11:09.480
**Susan Daniels**: After discussing another round of voting will happen, and after multiple rounds of voting and discussion, the group should eventually get closer to consensus through logic and reasoning and without bias.

55
00:11:10.200 --> 00:11:21.000
**Susan Daniels**: A root cause analysis is a helpful technique for getting to the bottom of an issue, it can be very easy to accidentally address the symptoms without digging deeper into why they're happening.

56
00:11:21.630 --> 00:11:26.430
**Susan Daniels**: let's explore some of the techniques that can be used to find the root cause of an issue.

57
00:11:27.060 --> 00:11:34.140
**Susan Daniels**: One is the idea of what if thinking, do you ever get accused of worrying too much or being a What if thinker.

58
00:11:34.620 --> 00:11:42.390
**Susan Daniels**: In my opinion, this can make you an excellent risk manager good risk management begins with risk identification.

59
00:11:42.690 --> 00:11:51.870
**Susan Daniels**: Maybe you're managing a project and you think what happens if our vendor delivers late or what happens if one of our TEAM members has an extended absence.

60
00:11:52.320 --> 00:12:00.690
**Susan Daniels**: This is an important first step to proper risk planning what steps would you put in place to address either of these risks situations.

61
00:12:00.990 --> 00:12:05.820
**Susan Daniels**: The five Wise is another technique which really helps you to get to the cause of a problem.

62
00:12:06.360 --> 00:12:12.420
**Susan Daniels**: let's say that you're having trouble getting to meetings on time, you always seem to be running 10 minutes late.

63
00:12:12.840 --> 00:12:20.190
**Susan Daniels**: Your current strategy is to just apologize for being late, but unfortunately that isn't a cure for the problem.

64
00:12:20.700 --> 00:12:30.360
**Susan Daniels**: let's try the five Wise, you should start with, why was I late to the meeting today, maybe, the answer is, I was running late from the last meeting.

65
00:12:30.960 --> 00:12:40.140
**Susan Daniels**: Then you ask why were you running late from the last meeting because we seem to have trouble sticking to the agenda, why do you have trouble sticking to the agenda.

66
00:12:40.590 --> 00:12:51.570
**Susan Daniels**: Because some TEAM members have trouble getting to the point well you get the idea, by the time we've asked why five times we usually find some things that can be improved.

67
00:12:52.140 --> 00:13:02.280
**Susan Daniels**: As the leader of the meeting that always runs long I can create time blocks for each agenda item and a whole to those time blocks as a way of improving time management for the group.

68
00:13:02.610 --> 00:13:09.960
**Susan Daniels**: We can also start a parking lot for issues that we need to visit outside of the meeting, and if i'm an attendee of that meeting.

69
00:13:10.290 --> 00:13:17.610
**Susan Daniels**: I can make sure, at the start of the meeting to let everybody know that I have a hard stop and I may need to leave if the meeting runs long.

70
00:13:18.090 --> 00:13:27.210
**Susan Daniels**: cause and effect diagrams and checklists can also be very helpful we use them in quality, too, and we'll cover them in more detail in that course.

71
00:13:27.630 --> 00:13:37.740
**Susan Daniels**: A pre mortem is a technique, where the team can run a mock meeting to discuss a project that has failed at least hypothetically the project didn't actually happen, yet.

72
00:13:38.160 --> 00:13:48.120
**Susan Daniels**: The team will discuss all of the fictional ways that the project went off track and what led to the failure, and this may be helpful for identifying possible risks.

73
00:13:48.480 --> 00:13:57.510
**Susan Daniels**: The point of using these techniques, is to avoid analysis pitfall addressing the symptoms, or the proximate causes won't solve the problem.

74
00:13:57.900 --> 00:14:03.330
**Susan Daniels**: These systemic problems in a project will only cause more failures if they aren't addressed.

75
00:14:03.810 --> 00:14:12.030
**Susan Daniels**: The SWOT analysis technique is used by stakeholders to identify an organization strengths weaknesses opportunities and threats.

76
00:14:12.510 --> 00:14:24.990
**Susan Daniels**: strengths are positive attributes of an entity examples, could be a skilled workforce or an easily accessed retail location, with high foot traffic weaknesses are areas, the entity could improve.

77
00:14:25.290 --> 00:14:34.380
**Susan Daniels**: Like outdated technology opportunities or something to exploit such as another entity that holds market share, but maybe up for sale.

78
00:14:34.950 --> 00:14:46.920
**Susan Daniels**: threats are defined as something that could cause trouble for the business or the project a SWOT analysis has a variety of uses, including evaluating the current environment that the organization is operating in.

79
00:14:47.490 --> 00:15:00.480
**Susan Daniels**: The strengths and weaknesses are characteristics of our organization, they can be identified internally opportunities and threats can be identified through external analysis of the project environment.

80
00:15:01.500 --> 00:15:01.770
What.

81
00:15:19.560 --> 00:15:31.920
**Susan Daniels**: School So the first order of business when we're dealing with risk management is using a variety of tools to begin to identify the risks that are out there, the threats and the opportunities.

82
00:15:32.880 --> 00:15:36.780
**Susan Daniels**: But once we've collected them all, then, what right.

83
00:15:37.440 --> 00:15:52.560
**Susan Daniels**: Well, we have to classify them, we have to organize them much like after we collect all those requirements we've got to put them in some sort of framework some sort of structure so that we can track them and communicate them to others in an organized way.

84
00:15:53.310 --> 00:16:02.490
**Susan Daniels**: Well there's a couple of approaches on how we can classify our risks Okay, we can organize it by our.

85
00:16:03.510 --> 00:16:11.160
**Susan Daniels**: The effect based Okay, what is going to impact right, this is going to impact my scope my quality my cost my schedule right.

86
00:16:11.490 --> 00:16:29.130
**Susan Daniels**: You can categorize it that way, but it gets a little messy because something's do both budget and schedule right or it might be quality and schedule, so this one is OK for pretty small projects another way to look at it.

87
00:16:30.570 --> 00:16:40.920
**Susan Daniels**: Well, you can use this as a secondary approach i'd recommend right if you really want to i'm kind of look at your your project list of risks.

88
00:16:41.400 --> 00:16:48.000
**Susan Daniels**: By where they could have the most impact, but it wouldn't be my primary way i'd be like a secondary view that i'd look for.

89
00:16:48.900 --> 00:17:08.640
**Susan Daniels**: One of the easiest ways to organize your risk register the the collection of risk that you've identified is really from from whence they came right from where are they are rising right and that gives you a kind of a broader categorization that you can use.

90
00:17:10.620 --> 00:17:17.370
**Susan Daniels**: But when you're thinking of these risks and you're trying to identify your risks, there are.

91
00:17:18.450 --> 00:17:31.470
**Susan Daniels**: Four lenses that we look for the first one is unknown known we know about these risks just about anybody could name them right there the obvious scripts information is fully studied and understood.

92
00:17:32.220 --> 00:17:40.770
**Susan Daniels**: that's not overly helpful, you could probably sit at your desk yourself and do the known knowns but maybe a subject matter expert could help you.

93
00:17:42.270 --> 00:17:50.790
**Susan Daniels**: known or unknown, is that we know that this we know about, we don't have enough information, information that is.

94
00:17:51.510 --> 00:18:02.250
**Susan Daniels**: That is understood to exist, but it's not in the possession of the person seeking it Okay, I know of this but I don't know enough about it does that makes sense, these are my I.

95
00:18:03.000 --> 00:18:14.130
**Susan Daniels**: am aware of this could be a threat or where this could be an opportunity, but I don't have enough information to really move forward with it that's my known unknowns I know I don't know okay.

96
00:18:15.150 --> 00:18:18.480
**Susan Daniels**: You were unknown unknowns okay.

97
00:18:19.560 --> 00:18:32.520
**Susan Daniels**: Nobody saw this common right wasn't even looking just unknown unknown, you know you don't know what you don't know okay that's that category, those are the whole surprises that happen.

98
00:18:34.020 --> 00:18:44.880
**Susan Daniels**: unknown knowns okay information that an individual or Organization has in its possession Okay, but whose existence or value is not been realized.

99
00:18:46.230 --> 00:18:58.200
**Susan Daniels**: What am I talking about here right pmi is basically saying this is people know these risks and desist, but if you don't ask they're not telling.

100
00:18:59.850 --> 00:19:12.810
**Susan Daniels**: So that's kind of a fatal mistake me that can happen, so what you want to try to do when you're doing risk identification it's easy to get the no nose.

101
00:19:13.080 --> 00:19:33.120
**Susan Daniels**: And even you can be given tips and leads on hey go after this I know you don't know much but go find more these known unknowns but where you have to be on top of your game i'm ahead of everyone is really be sure you're asking.

102
00:19:34.680 --> 00:19:36.750
**Susan Daniels**: A broad array of people.

103
00:19:37.920 --> 00:19:51.210
**Susan Daniels**: About risk identification risk identification is not something that is done quickly and with like check the box I held a meeting one meeting i'm done and all you have is a perspective of few people.

104
00:19:52.680 --> 00:20:03.720
**Susan Daniels**: Surveys questionnaires on are good tools, because you can distribute it to a broad amount of people, and you can get a lot of information back.

105
00:20:04.110 --> 00:20:18.540
**Susan Daniels**: So I encourage you to really think about who's on your distribution list that could give you insights give you tips help you figure out, you know it could really blow up and go wrong.

106
00:20:19.200 --> 00:20:32.940
**Susan Daniels**: If you just asked me, you know I know this is a problem that could happen, but if you don't ask, I have no way of telling you so that's the kind of mistake that i'm this whole segment is trying to tell you, be careful of this.

107
00:20:37.380 --> 00:20:48.150
**Susan Daniels**: So this is a checklist right on how to on how to identify key word here interactively interactively.

108
00:20:49.920 --> 00:21:01.950
**Susan Daniels**: When should do your your first robust effort at risk identification do not think is one and done Okay, you have your risk register.

109
00:21:02.370 --> 00:21:15.420
**Susan Daniels**: And at least weekly, preferably more often if you've got a really high risk project going on, you review the risk register and you asking questions like.

110
00:21:16.380 --> 00:21:32.820
**Susan Daniels**: Has any of these risks retired by that we mean has the window of his potential impact past right can we retire this risk, because it can no longer impact us love to retire risks.

111
00:21:34.500 --> 00:21:53.520
**Susan Daniels**: You also asked of the risk that we still think can impact us at this time, you kind of have a window of risks that are in play at time at this window of time in your project has the potential probability of it happening increased or decreased.

112
00:21:54.600 --> 00:22:04.740
**Susan Daniels**: I mean we initially thought it was a high probability, but now it's probably not as high as we thought in the beginning, things have changed or vice versa.

113
00:22:05.490 --> 00:22:15.360
**Susan Daniels**: We didn't think it could be very high, but you know there's a lot of signs that that's got a higher probability of happening, do we need to adjust the probability.

114
00:22:16.170 --> 00:22:26.010
**Susan Daniels**: During the same risk reassessment meeting you also want to ask Okay, if the probability remains the same, or has changed, will the impact be.

115
00:22:26.940 --> 00:22:40.080
**Susan Daniels**: best known same as more or less than we originally thought and sometimes people will update what the impact is more than the probability, but sometimes they'll give you insights that this could be worse than we thought.

116
00:22:41.700 --> 00:22:59.070
**Susan Daniels**: So you also want to look at um can we retire risk has anything changed and then this is what makes risk identification iterative, are there any new risks that we need to add to the register.

117
00:23:00.450 --> 00:23:12.270
**Susan Daniels**: keep asking that should be a question you're asking on a regular basis in your team meetings I make it an agenda item every meeting I have okay.

118
00:23:13.020 --> 00:23:20.160
**Susan Daniels**: it's just to kind of have an opening question you know hello, let everybody introduce themselves in the room, today, this is the agenda.

119
00:23:20.430 --> 00:23:38.610
**Susan Daniels**: Okay, and then we kind of go through the entire agenda at the bottom of the agenda, it says next steps and follow ups and then the last question before we leave is, are there any new risks that you're aware of for this project that you need to talk to me about after the meeting.

120
00:23:39.990 --> 00:23:52.110
**Susan Daniels**: Hello yeah yeah i'm so glad you reminded me Susan and they will walk out of the room with me and down the hall telling me about risks so just make it part of your culture.

121
00:23:52.800 --> 00:24:03.690
**Susan Daniels**: kind of known for being risk sensitive and risk aware that doesn't mean you're afraid cat or that you're a nervous person, it means i'm.

122
00:24:04.560 --> 00:24:18.750
**Susan Daniels**: alert Okay, of what could potentially happen your senior leadership your sponsors don't like surprises, any more than you do, but if you can catch it early you can work it right.

123
00:24:22.920 --> 00:24:26.070
**Susan Daniels**: i'm just looking at the chat before I move on.

124
00:24:29.550 --> 00:24:39.180
**Susan Daniels**: Do we need to think about a mitigation plan for risk or contingency plan as part of our risk registry you're ahead of the curve, for me, yes we'll be talking about.

125
00:24:39.900 --> 00:24:49.170
**Susan Daniels**: Risk response strategies and just a couple of minutes right and we'll also be talking about contingency planning for the risks as they arrives.

126
00:24:51.420 --> 00:25:00.240
**Susan Daniels**: On eddie you're asking about the different risk response strategies you're just ahead of the curve here a little bit I will be addressing that's.

127
00:25:04.410 --> 00:25:04.890
**Susan Daniels**: Okay.

128
00:25:06.540 --> 00:25:13.860
**Susan Daniels**: So we've identified all these risks yes right we've we've gone through and got.

129
00:25:15.180 --> 00:25:22.170
**Susan Daniels**: we've held meetings we've done questionnaires we pulled people right we've got we've used different tools we've done.

130
00:25:23.310 --> 00:25:31.800
**Susan Daniels**: The The root cause analysis of some things that could go wrong Okay, but our risk created equally.

131
00:25:33.270 --> 00:25:49.590
**Susan Daniels**: I dare say no okay some risks have more potential impact, some people some risk have higher probability, and when you put the probability and the impact together some are more.

132
00:25:51.060 --> 00:26:05.610
**Susan Daniels**: Large meeting with small risk okay there's different ways that we qualify the risks, so that eventually we know how to prioritize the risks and that's where we're headed i'm gonna go ahead and on off screen over here.

133
00:26:06.870 --> 00:26:10.950
**Susan Daniels**: TEE up the quality qualitative so we may be thinking look.

134
00:26:12.090 --> 00:26:15.060
**Susan Daniels**: And i'm going to drag it over give me a minute.

135
00:26:18.330 --> 00:26:22.230
**Susan Daniels**: So we may be thinking look at all of the things that can go wrong.

136
00:26:23.940 --> 00:26:25.350
**Susan Daniels**: And the qualitative risks.

137
00:26:31.860 --> 00:26:36.870
**Susan Daniels**: Once a project team has identified risks for the project, they may feel overwhelmed.

138
00:26:37.260 --> 00:26:43.440
**Susan Daniels**: While risk includes both threats and opportunities, most of the time it's the threats that get our attention.

139
00:26:43.770 --> 00:26:49.560
**Susan Daniels**: So we may be thinking look at all of the things that can go wrong with this project should we even do it.

140
00:26:49.950 --> 00:26:57.540
**Susan Daniels**: Just because we have a long list of potential threats doesn't mean that every single one of them requires our attention, or that we need to take action.

141
00:26:57.810 --> 00:27:08.190
**Susan Daniels**: The project team would rarely have enough resources to address every risk so, which ones, do we pay attention to this is where qualitative risk analysis comes in handy.

142
00:27:08.730 --> 00:27:17.610
**Susan Daniels**: Qualitative risk analysis is a technique to rank and prioritize risks and also to categorize them let's start with the risk categories.

143
00:27:17.940 --> 00:27:23.220
**Susan Daniels**: Examples of risk categories could be technical organizational and external.

144
00:27:23.670 --> 00:27:34.260
**Susan Daniels**: Examples of technical risk might include relying on new or unproven technology or being involved in a changing industry or with changing technical standards.

145
00:27:34.800 --> 00:27:45.960
**Susan Daniels**: Examples of organizational risks might include poor project prioritization inadequate or inconsistent funding or conflict with other projects for resources.

146
00:27:46.470 --> 00:27:53.670
**Susan Daniels**: And examples of external risks could be legal or regulatory changes or whether in natural disasters.

147
00:27:54.180 --> 00:27:59.670
**Susan Daniels**: The project team can come up with its own categories of risks and tailor this analysis as needed.

148
00:28:00.330 --> 00:28:06.840
**Susan Daniels**: Personally, I prefer to categorize risks, according to the knowledge areas of a project that need to be planned and managed.

149
00:28:07.260 --> 00:28:14.280
**Susan Daniels**: So this could include scope related risks procurement related risks resources related risks and so on.

150
00:28:14.730 --> 00:28:23.880
**Susan Daniels**: There is no rule as to which categories, you have to use, but it is helpful to group similar risks together because they can be managed better that way.

151
00:28:24.630 --> 00:28:31.950
**Susan Daniels**: Have you ever noticed that we can be pretty subjective in the way we describe things if you asked three people about the weather today.

152
00:28:32.340 --> 00:28:40.620
**Susan Daniels**: One person might say it's hot outside the second person might say it's cold for them, and the third person might not really have an opinion at all.

153
00:28:41.160 --> 00:28:48.960
**Susan Daniels**: The same is true for risk, if you think about your friends and family, you can probably identify the warrior or the What if thinker in the group.

154
00:28:49.230 --> 00:28:57.600
**Susan Daniels**: And you can probably also identify the person who doesn't think any risk is a big deal So how do we bridge these gaps in personal opinion.

155
00:28:57.870 --> 00:29:06.270
**Susan Daniels**: When it comes to risks, we need to establish consistency in the way we talk about them and that's where the definition of impact scales comes in handy.

156
00:29:06.750 --> 00:29:19.200
**Susan Daniels**: We should sit down together as a team and discuss what it means, if we experience a 10% increase in project costs, the definition of impact scales creates a standard that the team can all agree upon.

157
00:29:19.710 --> 00:29:24.870
**Susan Daniels**: And this document can be tailored it doesn't have to be five categories from very low to very high.

158
00:29:25.290 --> 00:29:32.460
**Susan Daniels**: You could just have low medium or high or whatever the team prefers what is important is to have the conversation.

159
00:29:33.120 --> 00:29:39.810
**Susan Daniels**: When determining which risks require our attention, we can perform a risk probability and impact assessment.

160
00:29:40.080 --> 00:29:52.560
**Susan Daniels**: and assign a combined risk rating for each of the risks identified in the risk register the assessment is typically performed by subject matter experts who are familiar with the work and with the types of risks.

161
00:29:53.070 --> 00:30:05.850
**Susan Daniels**: The risk rating or score is calculated by multiplying the probability, which is the likelihood that the risk will occur with the impact which is the effect on the project objectives that the risk could have.

162
00:30:06.180 --> 00:30:18.690
**Susan Daniels**: Therefore, the risk rating equals probability times impact in some industries like insurance, there may be a lot of data and experience, so the probability and impact can be detailed.

163
00:30:19.200 --> 00:30:35.370
**Susan Daniels**: In smaller, less sophisticated projects, you could use just a low medium or high type scale, the risks that have a high and medium risk grading definitely require the project teams attention, while risks with a low rating may not require any action at all.

164
00:30:42.480 --> 00:30:43.410
interviews and.

165
00:30:49.530 --> 00:30:52.800
interviews and data gathering or a helpful way of quantum.

166
00:30:58.890 --> 00:31:11.040
**Susan Daniels**: not sure why it always starts to play again when I move it to the next screen, I have to work on that um so I got a good question that came in and it had to do with.

167
00:31:11.760 --> 00:31:19.530
**Susan Daniels**: um, how do we have the question, though, who is typically the decision authority on accepting threat risks.

168
00:31:20.040 --> 00:31:38.580
**Susan Daniels**: Well, to help us in that decision making process, you have to again, you were neutral, you are a political you were simply there to serve, you have to help your leadership, your governance group make objective fact based decisions.

169
00:31:39.810 --> 00:31:45.360
**Susan Daniels**: So barb just showed you on the screen that we come up with a.

170
00:31:46.950 --> 00:32:01.590
**Susan Daniels**: Basically, an assessment scale right and what does high moderate low or very high or very low mean right we come up with a green definitely definitions, so that gives some of the bias or.

171
00:32:02.340 --> 00:32:14.910
**Susan Daniels**: People who are outliers in their thinking extremist okay if we all have agreed upon scale on what high moderate low means, then we can use a really great tool.

172
00:32:16.650 --> 00:32:33.720
**Susan Daniels**: it's called a probability and impact matrix wow looks kind of very mathematical and for those of you that are big fans of math, this is a lot easier to use than it looks so just take a deep breath and i'll show you how to read it okay.

173
00:32:36.060 --> 00:32:53.730
**Susan Daniels**: Basically, we can have threats or opportunities right and we just went through for each one of our risks, we had identify what was the probability and what was the impact.

174
00:32:55.650 --> 00:33:04.290
**Susan Daniels**: This particular scale is found in the TIM Bach sixth edition under identify risk not under quantify risk.

175
00:33:05.820 --> 00:33:14.400
**Susan Daniels**: The way that you get the score is to simply match the probability with the impact give you an example.

176
00:33:15.030 --> 00:33:24.150
**Susan Daniels**: let's say that we are concerned about something that has very high impact Okay, but it's got a pretty low probability.

177
00:33:24.960 --> 00:33:35.790
**Susan Daniels**: it's got a very high impact so i'm going to be very high impact right over here in this column, but it's got a very low probability of happening.

178
00:33:36.600 --> 00:33:57.510
**Susan Daniels**: Like a forest fire get happen, or we could have um our vendor of choice go out of business super high impact, but very little probability is going to happen, so you look for matching very low probability with very high impact and you get a score of 0.8.

179
00:33:58.530 --> 00:34:21.300
**Susan Daniels**: let's go, conversely, very high probability of happening very low impact very high probability but very low impact so i'm going to be 0.5 so you just really read across and down until you match based upon your description criteria that everyone agreed to.

180
00:34:22.710 --> 00:34:26.880
**Susan Daniels**: But what's the value of this chart right what's the takeaway.

181
00:34:28.020 --> 00:34:39.300
**Susan Daniels**: Do you see where this is dark okay it's darker than the other segments, this is moderately dark it's kind of lightly tan and then the outside is closer to white.

182
00:34:41.310 --> 00:34:53.550
**Susan Daniels**: Generally, the things that get scores in the dark area have very have high impact high to very high impact, and they also have hide a very high probability.

183
00:34:54.120 --> 00:35:01.710
**Susan Daniels**: This is often referred to as the red zone, so any risks that we get scores in the red zone.

184
00:35:02.070 --> 00:35:17.460
**Susan Daniels**: Okay, would be things that we definitely need to put into our contingency plan right things that we need to have definitely need to have risk response strategies in place, and maybe even have contingency funds.

185
00:35:18.180 --> 00:35:27.720
**Susan Daniels**: built into our budget and built into our schedule, so the question then becomes famous Daniel where do these numbers come from, I know I don't understand.

186
00:35:29.250 --> 00:35:32.790
**Susan Daniels**: let's take something straightforward like i'm.

187
00:35:34.470 --> 00:35:41.970
**Susan Daniels**: Very high impact it gets a point eight oh that's what we've we've said that a very high is worth.

188
00:35:42.480 --> 00:36:04.740
**Susan Daniels**: But if it's a medium probability, which is point 05 you multiply the two together so half a point Oh, a would be point 04 we've seen probability times impact very high to medium, so this would be 0.40.

189
00:36:06.060 --> 00:36:21.030
**Susan Daniels**: The scores are basically multiplying the probability score times the impact score that's where they come from, but the important use of this document is it gives you two things it lets you know whether it falls in the red zone.

190
00:36:22.080 --> 00:36:26.520
**Susan Daniels**: These are things that we definitely things that are considered in my.

191
00:36:27.090 --> 00:36:37.860
**Susan Daniels**: language of business here where i've come from is called above the line, these things are above the line and are sure to get funding and my contingency plan.

192
00:36:38.370 --> 00:36:56.910
**Susan Daniels**: These the grayer ones are things that I may not have money set aside for Okay, but i'm going to have a plan in place anyway in case it happens i'm going to know who's responsible to do what when if a trigger event happens for this threat or opportunity.

193
00:36:58.920 --> 00:37:05.400
**Susan Daniels**: Anything that's a risk that has a score out here in the white zone, whether it's opportunity or the white zone in a threat.

194
00:37:06.120 --> 00:37:23.160
**Susan Daniels**: These are usually the response strategy is usually we're going to accept it, if it happens we'll deal with it, when in if it does, the response strategy is acceptance now, this is because these in the white zone generally have.

195
00:37:24.360 --> 00:37:36.240
**Susan Daniels**: Very low to moderate Okay, they have very low probability of happening or they have low impact which we can work in this live with our respond to, should it occur.

196
00:37:39.780 --> 00:37:49.290
**Susan Daniels**: In your notes, you need to put probability and impact matrix use for qualitative okay qualitative.

197
00:37:51.690 --> 00:37:52.740
**Susan Daniels**: risk management.

198
00:37:56.280 --> 00:38:02.070
**Susan Daniels**: But now let's go a little further, we have some tools that we use i'm.

199
00:38:03.330 --> 00:38:05.790
**Susan Daniels**: gonna I see that I have some questions let's see.

200
00:38:07.320 --> 00:38:09.000
**Susan Daniels**: Yes, I multiply the scores.

201
00:38:10.470 --> 00:38:19.350
**Susan Daniels**: As a percentage of total budget baseline about how much on an average does the PM budget on to address risk okay.

202
00:38:20.520 --> 00:38:27.540
**Susan Daniels**: There is kind of an industry norm Eduardo for how much to be contingency reserves.

203
00:38:28.080 --> 00:38:39.450
**Susan Daniels**: Generally it's about 10% you do your budget of what is going to cost okay for each activity remember how we went from this side of the screen to the other, we.

204
00:38:39.780 --> 00:38:47.040
**Susan Daniels**: know this could be depending on your camera and whether you have it flipped or not i'm basically went from right.

205
00:38:47.490 --> 00:38:59.970
**Susan Daniels**: To left activities, and then we summed it up for the work packages and then we added on the contention reserve on top, you have to build your budget first to get the project done just to get the work done.

206
00:39:00.810 --> 00:39:11.310
**Susan Daniels**: generally about 10% should be set aside for contingencies that's the minimum that's kind of the industry norm to have 10%.

207
00:39:11.760 --> 00:39:27.030
**Susan Daniels**: If it's high risk projects, I mean you've got a lot of risk identified in the red zone, and then you are going to need to maybe go up to 15% set aside available for.

208
00:39:28.440 --> 00:39:33.150
**Susan Daniels**: Contingency response you know, for your response strategies now.

209
00:39:34.350 --> 00:39:47.790
**Susan Daniels**: That money if it's not used Okay, sometimes at the end of the project right, it becomes bonus dollars, sometimes not depending on your business on domain and your corporation you work for.

210
00:39:49.230 --> 00:40:04.800
**Susan Daniels**: But that's somewhere around 10% would be the kind of the average of what this is based upon probably a couple of decades of being engaged in project management and going to project management.

211
00:40:06.630 --> 00:40:17.640
**Susan Daniels**: conventions okay being being involved in project management chapters and just kind of talking about contingency reserve so that's where i've seen it land.

212
00:40:19.050 --> 00:40:19.770
**Susan Daniels**: let's see.

213
00:40:22.800 --> 00:40:27.420
**Susan Daniels**: Why aren't probability and impact ranked equally um.

214
00:40:29.070 --> 00:40:37.230
**Susan Daniels**: They they are, but you may have low probability but high impact so it's really that you use the scale to figure out.

215
00:40:38.670 --> 00:40:48.720
**Susan Daniels**: What what it is because probability can be anything from high to low and impact can be from anything from high to low they're not equal, there are two perspectives that we find two axes.

216
00:40:50.880 --> 00:40:56.550
**Susan Daniels**: Okay Susan can you provide an example of a very high impact and very high probability opportunity.

217
00:40:58.020 --> 00:41:12.030
**Susan Daniels**: i'm going to throw that back to the group Okay, I know i've got over about 750 people online right now i'm sure someone can offer you an example of high probability high impact that that could occur.

218
00:41:13.350 --> 00:41:18.660
**Susan Daniels**: may be interesting to see from the different business domains what examples come through.

219
00:41:22.380 --> 00:41:26.820
**Susan Daniels**: So let's go ahead and talk about quantitative risk management.

220
00:41:28.350 --> 00:41:48.690
**Susan Daniels**: Quantitative risk management, using sophisticated tools okay to be able to do it and so therefore these tools are usually used on very large projects or complex ones things like NASA Boeing Department of Defense large large construction projects things like that.

221
00:41:51.420 --> 00:42:06.450
**Susan Daniels**: Okay, sometimes you have to do, quantitative risk analysis on when it's required by contract and the Federal Government is getting more and more embedded into pmi methodologies and tools and tech.

222
00:42:07.020 --> 00:42:22.050
**Susan Daniels**: into pmi best practices would be the best way to put it into pmi project management standards so we're seeing the federal government adopt and put into contracts, the need for quantitative risk analysis.

223
00:42:24.120 --> 00:42:28.290
**Susan Daniels**: So let me see if I can pull up the quantitative risk analysis.

224
00:42:44.220 --> 00:42:46.440
**Susan Daniels**: Now we we watch all the way through.

225
00:42:48.720 --> 00:42:54.210
interviews and data gathering or a helpful way of quantifying the possible cost.

226
00:43:00.990 --> 00:43:07.590
**Susan Daniels**: interviews and data gathering or a helpful way of quantifying the possible costs of risk events on your project.

227
00:43:07.980 --> 00:43:16.830
**Susan Daniels**: You can interview experienced project participants stakeholders and subject matter experts who can help you to estimate the costs of the risks.

228
00:43:17.190 --> 00:43:24.000
**Susan Daniels**: You can also gather estimates from data available in commercial databases, or from your risk management department.

229
00:43:24.330 --> 00:43:30.420
**Susan Daniels**: Your insurance agency may also be able to share the risk experience of projects that are similar to yours.

230
00:43:30.900 --> 00:43:35.850
**Susan Daniels**: These estimates will often be provided in ranges to reflect the uncertainty of the estimate.

231
00:43:36.210 --> 00:43:49.350
**Susan Daniels**: A low cost would be considered optimistic, while a high estimate would be considered pessimistic when you learn about estimates for cost and schedule you'll come across a technique called three point estimating or PR.

232
00:43:49.740 --> 00:43:54.660
**Susan Daniels**: will use the same concept to come up with a range of cost estimates for risk.

233
00:43:55.170 --> 00:44:08.070
**Susan Daniels**: When we have a subject matter expert available and that person knows the most likely outcome from their own experience, we can wait, the most likely estimate a little more than the others, and the outcome will be a graph.

234
00:44:08.820 --> 00:44:16.260
**Susan Daniels**: In this example interviewing relevant stakeholders helps determine the three point estimates for each wps element.

235
00:44:16.590 --> 00:44:23.850
**Susan Daniels**: In this example interviewing relevant stakeholders helps determine the three point estimates for each wps element.

236
00:44:24.510 --> 00:44:32.580
**Susan Daniels**: The likelihood of completing this project at or below the most likely estimate of $41 million is relatively small.

237
00:44:33.060 --> 00:44:40.200
**Susan Daniels**: The most likely estimate, because it was provided by subject matter expertise helps to skew the likelihood of the outcome.

238
00:44:40.590 --> 00:44:47.220
**Susan Daniels**: We cover three point estimates in more detail in the cost and schedule courses and you'll learn more about PR there.

239
00:44:47.760 --> 00:44:59.910
**Susan Daniels**: The most important thing to know now is that risk estimates require the assistance of subject matter, experts and that the estimates are reported in ranges that represent the uncertainty of the outcome.

240
00:45:00.750 --> 00:45:06.780
**Susan Daniels**: and influence diagram is a graphical aid used to assist decision making under uncertainty.

241
00:45:07.110 --> 00:45:20.700
**Susan Daniels**: They can be used to identify cause and effect relationships between and among multiple project risks and events in this example we can consider that project estimates don't just come directly from the estimates of each activity.

242
00:45:21.060 --> 00:45:30.600
**Susan Daniels**: there's much more to it, the project activities create deliverables and there are possible risks related to those deliverables such as defects.

243
00:45:31.080 --> 00:45:40.080
**Susan Daniels**: These risk conditions can also impact project estimates, which is the reason they're included in an influence diagram for project estimates.

244
00:45:40.530 --> 00:45:47.100
**Susan Daniels**: This is a very simplified model of an influence diagram, but it does demonstrate the basic concept.

245
00:45:47.790 --> 00:45:56.760
**Susan Daniels**: A sensitivity analysis, also known as a tornado diagram is a bar chart that models which risks have the highest impact on the project.

246
00:45:57.120 --> 00:46:06.720
**Susan Daniels**: The risks are ranked by relative importance with the projects that have the highest risk on top, as well as the projects that have the highest range of uncertainty.

247
00:46:06.990 --> 00:46:17.970
**Susan Daniels**: Some risks are only threats with a negative impact, for instance, if there is a warehouse fire that destroys our inventory there isn't really a good side to that risk event.

248
00:46:18.210 --> 00:46:29.160
**Susan Daniels**: So risk, one could depict a situation where we only have negative impact or threat risk number three shows only an opportunity or a positive impact, to the project.

249
00:46:29.460 --> 00:46:39.030
**Susan Daniels**: Maybe you're working with a vendor and your volume of business makes you eligible for rebate there isn't really a downside to this, but what about risk to.

250
00:46:39.420 --> 00:46:44.670
**Susan Daniels**: It shows both a negative and a positive side, sometimes that will happen.

251
00:46:45.150 --> 00:46:54.270
**Susan Daniels**: Perhaps you have the chance to enter into a new business venture there's a lot of opportunity to profit, but at the same time it's possible that you could lose your investment.

252
00:46:54.600 --> 00:47:06.900
**Susan Daniels**: So the range of possibilities stretches from the negative outcome, all the way to a positive outcome, the sensitivity analysis is a great visual tool for comparing different risks along a scale.

253
00:47:07.380 --> 00:47:22.050
**Susan Daniels**: A Monte Carlo simulation is a type of probability analysis, the simulation uses computer modeling to determine a range of possibilities, the simulation is run several thousand times until a range of probabilities becomes clear.

254
00:47:22.350 --> 00:47:34.140
**Susan Daniels**: This technique is used quite often with storm tracking, for example, models will be used to predict when and where a hurricane could make landfall and also how strong the storm will be when it hits.

255
00:47:34.650 --> 00:47:44.190
**Susan Daniels**: Among the different models there's a range of possibilities, also the further into the future, the event is the wider the range of uncertainty will be.

256
00:47:44.520 --> 00:47:54.810
**Susan Daniels**: Monte Carlo is not a formula that a person can calculate it's a complex algorithm and the scenario is run thousands of times to get the range of scenarios.

257
00:47:55.080 --> 00:48:02.820
**Susan Daniels**: You can use a Monte Carlo simulation to determine the probability that you'll complete your project within the planned budget or schedule.

258
00:48:11.190 --> 00:48:13.890
interviews and data gathering or a helpful way.

259
00:48:20.910 --> 00:48:38.730
**Susan Daniels**: And we got a question in the chat from Michael about this Monte Carlo of software, let us to be extremely expensive and had to be run on very large supercomputers so was the exclusive algorithm of the big big companies.

260
00:48:39.270 --> 00:48:45.870
**Susan Daniels**: um but today with today's technology and computing power and a lot of.

261
00:48:47.040 --> 00:48:57.960
**Susan Daniels**: open source software, one can find Monte Carlo software packages for a very reasonable fee out there in the market so it's now something you can use.

262
00:48:58.920 --> 00:49:06.900
**Susan Daniels**: If you choose to to show on the range of probability that something is likely to happen.

263
00:49:07.260 --> 00:49:20.040
**Susan Daniels**: I think the hurricane is an act, the hurricane map is an excellent illustration of Monte Carlo Okay, but it can also be used to finding what is our highest probability of when something's going to be done.

264
00:49:21.000 --> 00:49:31.650
**Susan Daniels**: Given all of the estimates that you have what's the highest probability of what this is going to cost given on the input that's put in, so it can be used in a variety of ways.

265
00:49:34.590 --> 00:49:42.870
**Susan Daniels**: Oh, thank you for sharing Kimberly Monte Carlo simulations can be run and the xml data analysis add on virtual need.

266
00:49:43.440 --> 00:49:54.120
**Susan Daniels**: expertise to do it yourself right it's available now on your desktop with capabilities, with a small add on right it's it's just amazing where technology has taken us today.

267
00:49:55.800 --> 00:50:02.430
**Susan Daniels**: So at this point, I want you to say we've collected them we've categorized them we've.

268
00:50:03.180 --> 00:50:20.670
**Susan Daniels**: Come to consensus on his probability and impact we've scored them we figured out, what are the highest priority moderate priority those that will watch and monitor based upon our probability impact matrix and now, what are we going to do about it.

269
00:50:21.690 --> 00:50:29.010
**Susan Daniels**: What are we going to do about all these risks that we've identified, well, we have to come up with response strategies.

270
00:50:30.360 --> 00:50:50.280
**Susan Daniels**: there's a whole video coming up on response strategies um but i'm just going to introduce a concept here about respond, let me just open up a give me a chance to open a whiteboard and see if I can find the whiteboard.

271
00:50:52.140 --> 00:50:53.370
**Susan Daniels**: chat polls.

272
00:50:54.840 --> 00:50:57.090
**Susan Daniels**: where's my whiteboard more.

273
00:51:00.780 --> 00:51:01.230
**Susan Daniels**: huh.

274
00:51:04.080 --> 00:51:06.390
**Susan Daniels**: susan's not seeing the up the whiteboard.

275
00:51:08.550 --> 00:51:18.690
**Susan Daniels**: Interesting alright so i'm just going to have to verbally tell you this, that your pen and pencil ready Okay, I want you to write plan a.

276
00:51:21.000 --> 00:51:29.880
**Susan Daniels**: plan a equals the project plan which you intend to happen Okay, this is the plan.

277
00:51:31.560 --> 00:51:33.360
**Susan Daniels**: Plan B.

278
00:51:35.880 --> 00:51:47.160
**Susan Daniels**: that's your contingency plan that is if a risk is identified what you're going to do about it okay Plan B for this work package.

279
00:51:49.710 --> 00:51:52.770
**Susan Daniels**: contingency plan Plan B equals your contingency plan.

280
00:51:53.880 --> 00:51:59.700
**Susan Daniels**: plan C okay is your fallback plan.

281
00:52:00.750 --> 00:52:14.700
**Susan Daniels**: plan C is your fallback plan if if your contingency plan doesn't work do you have a backup plan to your contingency plan, what are you going to fall back to okay.

282
00:52:15.150 --> 00:52:29.130
**Susan Daniels**: If that contingency plan doesn't work and trust me for some of your really mission critical work packages things on your critical path, perhaps you're definitely going to have.

283
00:52:29.820 --> 00:52:39.000
**Susan Daniels**: Your plan your contingency plan and a fallback plan, if there are things threatening i'm sort of mission critical components.

284
00:52:40.650 --> 00:52:46.050
**Susan Daniels**: But then we have another term called work around okay.

285
00:52:47.070 --> 00:52:48.150
**Susan Daniels**: Work around.

286
00:52:50.640 --> 00:52:56.520
**Susan Daniels**: The workaround is response when there is no plan.

287
00:52:57.690 --> 00:53:16.770
**Susan Daniels**: it's your response when there is no plan it's one of those unknown unknowns that popped up nobody saw it coming don't have a plan for this Okay, you are now doing a workaround plan now jargon is you know fire drill.

288
00:53:18.540 --> 00:53:22.770
**Susan Daniels**: stomping out fires, crisis management okay.

289
00:53:23.970 --> 00:53:29.940
**Susan Daniels**: we're implementing workaround plans when we're doing those sorts of things no response available.

290
00:53:32.460 --> 00:53:51.300
**Susan Daniels**: Why, I want you to know those terms, is because you may see them as choices on your exam okay and what's the difference between contingency plan in a fallback plan what's the difference between your contingency plan in a workaround plan Okay, you need to know you just need to know.

291
00:53:53.070 --> 00:54:04.020
**Susan Daniels**: There is an expression here on the last paragraph review secondary risks Okay, these are wrist it could result as implementing our risk response.

292
00:54:06.660 --> 00:54:19.710
**Susan Daniels**: Sometimes, our contingency plan is not without its own risks right i'm in my bring its own set of troubles with it, those are called the secondary was you're trying to respond to the risk.

293
00:54:20.130 --> 00:54:34.680
**Susan Daniels**: That manifested okay became an issue and your contingency response plan well it's got issues or it's it's got risks of its own, and those are called secondary risks, let me give you this example.

294
00:54:36.150 --> 00:55:03.510
**Susan Daniels**: If you are familiar with marvel as a film genre and you're familiar with the League and if you happen to be familiar with Hulk okay as a character um Hulk comes joins the fight so you know, the response plan is to fight but doesn't help bring his own set of risk, what does Hulk do.

295
00:55:05.820 --> 00:55:25.650
**Susan Daniels**: yeah i'm talking about the avengers Hulk smashes Okay, so he brings his own downside right so that's an example of putting in a plan with secondary risks, let me give you an example of the plan the contingency plan and a fallback plan.

296
00:55:27.510 --> 00:55:29.070
**Susan Daniels**: picture this, if you will.

297
00:55:31.050 --> 00:55:32.610
**Susan Daniels**: i'm just looking at the.

298
00:55:33.750 --> 00:55:35.250
**Susan Daniels**: At the chat sorry.

299
00:55:36.630 --> 00:55:37.470
**Susan Daniels**: So.

300
00:55:38.580 --> 00:55:45.060
**Susan Daniels**: picture this, if you will, you have a mission critical component for some.

301
00:55:46.260 --> 00:55:54.990
**Susan Daniels**: thing that you were building and it's being manufactured in the in a mountain region Okay, and the winter.

302
00:55:56.580 --> 00:56:04.500
**Susan Daniels**: And you have to get it from that mountain region, all the way to the coast, where you are okay and it's 1000 miles away.

303
00:56:05.790 --> 00:56:22.740
**Susan Daniels**: it's winter it's in the mountains, can you think of a risk that might happen if i'm trying to drive that part from the mountains down the range it to the coast right bad weather snow.

304
00:56:23.970 --> 00:56:28.410
**Susan Daniels**: can make the roads and passable pretty known known risk.

305
00:56:30.210 --> 00:56:41.010
**Susan Daniels**: So i'm going to create a contingency plan for that cuz it does know right, so my contingency plan is, if there is a forecast of more than.

306
00:56:42.120 --> 00:57:00.390
**Susan Daniels**: Four or five inches of snow during this window of time that I needed to be transported then that's the trigger for me to move for it to go by rail, because trains can run when the on the roads are I stuff but the trains can keep plowing through a reasonably.

307
00:57:01.830 --> 00:57:03.840
**Susan Daniels**: Low light to moderate snowfall.

308
00:57:05.970 --> 00:57:12.150
**Susan Daniels**: Now, if I have a blizzard that happens, which I could actually get a blizzard sometimes.

309
00:57:12.420 --> 00:57:22.680
**Susan Daniels**: If the snows too deep even the trains won't go through the mountain so now, I need a fallback plan, because this is a mission critical component, I have to have it.

310
00:57:23.040 --> 00:57:36.150
**Susan Daniels**: it's a fail point on my project if I don't get it, so I have my plan take it by road, I have my contingency plan take it by train, I have a fallback plan to take it.

311
00:57:36.570 --> 00:57:47.520
**Susan Daniels**: To the airport now, yes, it might sit on the tarmac until the snow stops falling, but I can fly it pretty fast once they do the wings.

312
00:57:49.530 --> 00:57:54.720
**Susan Daniels**: Well, that was the plan that was the contingency plan that was the fallback plan.

313
00:57:56.310 --> 00:58:18.060
**Susan Daniels**: There was secondary risk that comes into play by trying to get that part from the manufacturer through blizzard conditions to the airport Okay, I was we were literally putting the driver at risk to transport the equipment to the airport or at least we thought.

314
00:58:19.500 --> 00:58:29.970
**Susan Daniels**: Little did we know and we built money into the on the budget to be able to incentivize someone to drive it during those adverse conditions.

315
00:58:30.270 --> 00:58:50.490
**Susan Daniels**: Because, quite honestly, we're from Florida so blizzard scare us okay Florida is a sunny southern part of the United States, they lots of sunshine doesn't snow here, so we incentivize we've got money to incentivize someone to drive that equipment from the manufacturer the airport.

316
00:58:52.080 --> 00:58:59.370
**Susan Daniels**: Little did we know they have these huge trucks with all kinds of snow equipment to be able to transverse the mountains.

317
00:59:00.060 --> 00:59:10.200
**Susan Daniels**: With these okay so somebody made a really nice little bonus that day for just driving across town in their big heavy duty snow trucks, but um.

318
00:59:11.190 --> 00:59:23.580
**Susan Daniels**: The whole takeaway from this is that's what Plan contingency plan fallback plan Okay, and what a workaround plan is we don't have a response for this and what a secondary risk is.

319
00:59:25.710 --> 00:59:36.150
**Susan Daniels**: This line is a nice summary slide that tells you what the video is about to tell you, and when we come back we're going to watch that video and we'll.

320
00:59:36.540 --> 00:59:52.590
**Susan Daniels**: Well, you know I got long enough, I think, to watch the video yeah i'm actually not going to respect that we're getting close to the bottom of the hour, we only have a couple of minutes to the bottom, and this is a six minute video and I kind of like to stay on cadence.

321
00:59:53.970 --> 01:00:07.920
**Susan Daniels**: So we're going to i'm going to go ahead and go on break i'm going to stop and we come back we'll open the next segment, with the risk response strategy video See you in a couple of minutes.

